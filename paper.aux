\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background and Related Work}{1}}
\citation{PattersonGibson17}
\citation{zeiler2014visualizing}
\citation{hochreiter1997long}
\citation{riedmiller1993direct}
\citation{PattersonGibson17}
\citation{abadi2016tensorflow}
\citation{jia2014caffe}
\citation{collobert2002torch}
\citation{gitcntk}
\citation{websitedl4j}
\citation{websitekeras}
\citation{websitemxnet}
\citation{shi2016benchmarking}
\citation{qi2016paleo}
\citation{PattersonGibson17}
\@writefile{toc}{\contentsline {section}{\numberline {III}Choice of Neural Network}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}What kind of networks to use}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}Simple Networks}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Deep Networks}{2}}
\citation{PattersonGibson17}
\citation{junyanz2017}
\citation{zhu2016generative}
\citation{gittesttensorflow}
\citation{sutskever2011generating}
\citation{zhong2016diversified}
\citation{soltau2016neural}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Simple network Nodes}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Deep networks Nodes}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Specific deep networks}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Training time(in s) for a mini-batch of size 64 of a Fully Connected Network(FCN) trained with synthetic data on a i7-3820 with 4-physical cores using 3 different commonly used Deep Learning frameworks\cite  {shi2016benchmarking}}}{3}}
\newlabel{fig_ttfcn}{{I}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Choice of Processing Units}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}CPU}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}1}Single-thread}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}2}Multi-thread}{3}}
\citation{shi2016benchmarking}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{sastre2017scalability}
\citation{sastre2017scalability}
\citation{wang2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}GPU}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Multi-GPU}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}1}Parallelism between GPUs}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}2}Synchronization between GPUs}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Time per mini-batch in s compared between different amount of GPUs used\cite  {shi2016benchmarking}}}{4}}
\newlabel{fig_m_gpu}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Time (in h) of different asynchronous configurations until reaching destined error rate(loss)\cite  {sastre2017scalability}}}{4}}
\newlabel{fig_cl_gpus}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}GPU-Clusters}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}1}Multiple GPU-Clusters}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}2}Parallelism of multiple GPU-Clusters}{4}}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{yang2010gpgpu}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}3}Synchronization of multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Large scale challenges and Small scale challenges}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Small scale challenges}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Large scale challenges}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces One example configuration consisting of a CPU and 4 connected GPU units for training the neural network.}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces One example configuration consisting of a communication server and 4 connected GPU-Clusters each containing 4 GPUs.}}{5}}
\citation{nvidiagpudirect2017}
\citation{nvidiagpudirect2017}
\citation{yang2010gpgpu}
\bibstyle{IEEEtran}
\bibdata{paper}
\bibcite{PattersonGibson17}{1}
\bibcite{zeiler2014visualizing}{2}
\bibcite{hochreiter1997long}{3}
\bibcite{riedmiller1993direct}{4}
\bibcite{abadi2016tensorflow}{5}
\bibcite{jia2014caffe}{6}
\bibcite{collobert2002torch}{7}
\bibcite{gitcntk}{8}
\bibcite{websitedl4j}{9}
\bibcite{websitekeras}{10}
\bibcite{websitemxnet}{11}
\bibcite{shi2016benchmarking}{12}
\bibcite{qi2016paleo}{13}
\bibcite{junyanz2017}{14}
\bibcite{zhu2016generative}{15}
\bibcite{gittesttensorflow}{16}
\bibcite{sutskever2011generating}{17}
\bibcite{zhong2016diversified}{18}
\bibcite{soltau2016neural}{19}
\bibcite{sastre2017scalability}{20}
\bibcite{wang2016deep}{21}
\bibcite{yang2010gpgpu}{22}
\bibcite{nvidiagpudirect2017}{23}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
