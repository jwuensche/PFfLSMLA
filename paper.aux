\relax 
\citation{deng2014deep}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Popularity in \% of the term "deep learning" on Google in relation to maximum peak, plotted with the Google Trends tool provided by Google}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background and Related Work}{1}}
\citation{PattersonGibson17}
\citation{zeiler2014visualizing}
\citation{hochreiter1997long}
\citation{riedmiller1993direct}
\citation{PattersonGibson17}
\citation{abadi2016tensorflow}
\citation{jia2014caffe}
\citation{collobert2002torch}
\citation{gitcntk}
\citation{websitedl4j}
\citation{websitekeras}
\citation{websitemxnet}
\citation{shi2016benchmarking}
\citation{qi2016paleo}
\@writefile{toc}{\contentsline {section}{\numberline {III}Performance impacting choices}{2}}
\citation{PattersonGibson17}
\citation{PattersonGibson17}
\citation{junyanz2017}
\citation{zhu2016generative}
\citation{gittesttensorflow}
\citation{sutskever2011generating}
\citation{zhong2016diversified}
\citation{soltau2016neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Choice of Neural Network}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}Shallow Networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Deep Networks}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Shallow network node amount}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Deep networks node amount}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Specific deep networks}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Choice of Processing Units}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-F}}CPU}{3}}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Training time(in s) for a mini-batch of size 64 of a Fully Connected Network(FCN) trained with synthetic data on a i7-3820 with 4-physical cores using 3 different commonly used Deep Learning frameworks\cite  {shi2016benchmarking}}}{4}}
\newlabel{fig_ttfcn}{{I}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-F}1}Single-thread}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-F}2}Multi-thread}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-G}}GPU}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-H}}Multi-GPU}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Time per mini-batch in s compared between different amount of GPUs used\cite  {shi2016benchmarking}}}{4}}
\newlabel{fig_m_gpu}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-H}1}Parallelism between GPUs}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-H}2}Synchronization between GPUs}{4}}
\citation{sastre2017scalability}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{yang2010gpgpu}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Time (in h) of different asynchronous configurations until reaching destined error rate(loss)\cite  {sastre2017scalability}}}{5}}
\newlabel{fig_cl_gpus}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-I}}GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}1}Multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}2}Parallelism of multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}3}Synchronization of multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Large scale challenges and Small scale challenges}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces One example configuration consisting of a CPU and 4 connected GPU units for training the neural network.}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Small scale challenges}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Large scale challenges}{5}}
\citation{nvidiagpudirect2017}
\citation{nvidiagpudirect2017}
\citation{yang2010gpgpu}
\bibstyle{IEEEtran}
\bibdata{paper}
\bibcite{deng2014deep}{1}
\bibcite{PattersonGibson17}{2}
\bibcite{zeiler2014visualizing}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces One example configuration consisting of a communication server and 4 connected GPU-Clusters each containing 4 GPUs.}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
\bibcite{hochreiter1997long}{4}
\bibcite{riedmiller1993direct}{5}
\bibcite{abadi2016tensorflow}{6}
\bibcite{jia2014caffe}{7}
\bibcite{collobert2002torch}{8}
\bibcite{gitcntk}{9}
\bibcite{websitedl4j}{10}
\bibcite{websitekeras}{11}
\bibcite{websitemxnet}{12}
\bibcite{shi2016benchmarking}{13}
\bibcite{qi2016paleo}{14}
\bibcite{junyanz2017}{15}
\bibcite{zhu2016generative}{16}
\bibcite{gittesttensorflow}{17}
\bibcite{sutskever2011generating}{18}
\bibcite{zhong2016diversified}{19}
\bibcite{soltau2016neural}{20}
\bibcite{sastre2017scalability}{21}
\bibcite{wang2016deep}{22}
\bibcite{yang2010gpgpu}{23}
\bibcite{nvidiagpudirect2017}{24}
