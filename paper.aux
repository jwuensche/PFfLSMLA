\relax 
\citation{shi2016benchmarking}
\citation{qi2016paleo}
\citation{zoo}
\citation{zoo}
\citation{frank1957perceptron}
\citation{deeplearning101}
\citation{deng2014deep}
\citation{hinton2006fast}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces "A mostly complete chart of Neural Networks" by "The Asimov Institute" extract of only in this paper referenced networks\cite  {zoo}}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background and Related Work}{1}}
\citation{hinton2006fast}
\citation{PattersonGibson17}
\citation{zeiler2014visualizing}
\citation{hochreiter1997long}
\citation{riedmiller1993direct}
\citation{PattersonGibson17}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Popularity in \% of the term "deep learning" on Google in relation to maximum peak, plotted with the Google Trends tool provided by Google}}{2}}
\citation{abadi2016tensorflow}
\citation{jia2014caffe}
\citation{collobert2002torch}
\citation{gitcntk}
\citation{websitedl4j}
\citation{websitekeras}
\citation{websitemxnet}
\citation{PattersonGibson17}
\@writefile{toc}{\contentsline {section}{\numberline {III}Performance impacting choices}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Choice of Neural Network}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}Shallow Networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Deep Networks}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Shallow network node amount}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Deep networks node amount}{3}}
\citation{PattersonGibson17}
\citation{junyanz2017}
\citation{zhu2016generative}
\citation{gittesttensorflow}
\citation{sutskever2011generating}
\citation{zhong2016diversified}
\citation{soltau2016neural}
\citation{shi2016benchmarking}
\citation{sastre2017scalability}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Specific deep networks}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Choice of Processing Units}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-F}}CPU}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Training time(in s) for a mini-batch of size 64 of a Fully Connected Network(FCN) trained with synthetic data on a i7-3820 with 4-physical cores using 3 different commonly used Deep Learning frameworks\cite  {shi2016benchmarking}}}{4}}
\newlabel{fig_ttfcn}{{I}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-F}1}Single-thread}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-F}2}Multi-thread}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-G}}GPU}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-H}}Multi-GPU}{4}}
\citation{shi2016benchmarking}
\citation{shi2016benchmarking}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{sastre2017scalability}
\citation{wang2016deep}
\citation{sastre2017scalability}
\citation{wang2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Time per mini-batch in s compared between different amount of GPUs used\cite  {shi2016benchmarking}}}{5}}
\newlabel{fig_m_gpu}{{3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-H}1}Parallelism between GPUs}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-H}2}Synchronization between GPUs}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Time (in h) of different asynchronous configurations until reaching destined error rate(loss)\cite  {sastre2017scalability}}}{5}}
\newlabel{fig_cl_gpus}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-I}}GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}1}Multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}2}Parallelism of multiple GPU-Clusters}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-I}3}Synchronization of multiple GPU-Clusters}{5}}
\citation{yang2010gpgpu}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces One example configuration consisting of a CPU and 4 connected GPU units for training the neural network.}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Large scale challenges and Small scale challenges}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Small scale challenges}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Large scale challenges}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces One example configuration consisting of a communication server and 4 connected GPU-Clusters each containing 4 GPUs.}}{6}}
\citation{nvidiagpudirect2017}
\citation{nvidiagpudirect2017}
\citation{yang2010gpgpu}
\bibstyle{IEEEtran}
\bibdata{paper}
\bibcite{shi2016benchmarking}{1}
\bibcite{qi2016paleo}{2}
\bibcite{zoo}{3}
\bibcite{frank1957perceptron}{4}
\bibcite{deeplearning101}{5}
\bibcite{deng2014deep}{6}
\bibcite{hinton2006fast}{7}
\bibcite{krizhevsky2012imagenet}{8}
\bibcite{PattersonGibson17}{9}
\bibcite{zeiler2014visualizing}{10}
\bibcite{hochreiter1997long}{11}
\bibcite{riedmiller1993direct}{12}
\bibcite{abadi2016tensorflow}{13}
\bibcite{jia2014caffe}{14}
\bibcite{collobert2002torch}{15}
\bibcite{gitcntk}{16}
\bibcite{websitedl4j}{17}
\bibcite{websitekeras}{18}
\bibcite{websitemxnet}{19}
\bibcite{junyanz2017}{20}
\bibcite{zhu2016generative}{21}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
\bibcite{gittesttensorflow}{22}
\bibcite{sutskever2011generating}{23}
\bibcite{zhong2016diversified}{24}
\bibcite{soltau2016neural}{25}
\bibcite{sastre2017scalability}{26}
\bibcite{wang2016deep}{27}
\bibcite{yang2010gpgpu}{28}
\bibcite{nvidiagpudirect2017}{29}
